{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:27.563372Z","iopub.execute_input":"2024-05-05T03:48:27.563986Z","iopub.status.idle":"2024-05-05T03:48:27.931796Z","shell.execute_reply.started":"2024-05-05T03:48:27.563954Z","shell.execute_reply":"2024-05-05T03:48:27.931014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:28.276414Z","iopub.execute_input":"2024-05-05T03:48:28.277253Z","iopub.status.idle":"2024-05-05T03:48:30.364502Z","shell.execute_reply.started":"2024-05-05T03:48:28.277219Z","shell.execute_reply":"2024-05-05T03:48:30.363706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:30.366119Z","iopub.execute_input":"2024-05-05T03:48:30.366442Z","iopub.status.idle":"2024-05-05T03:48:34.689183Z","shell.execute_reply.started":"2024-05-05T03:48:30.366416Z","shell.execute_reply":"2024-05-05T03:48:34.688384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/playground-series-s4e5/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:34.690403Z","iopub.execute_input":"2024-05-05T03:48:34.690791Z","iopub.status.idle":"2024-05-05T03:48:35.981225Z","shell.execute_reply.started":"2024-05-05T03:48:34.690749Z","shell.execute_reply":"2024-05-05T03:48:35.980418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(),\n    \"Gradient Boosting\": GradientBoostingRegressor(),\n    \"Support Vector Machine\": SVR(),\n    \"XGBoost\": XGBRegressor(),\n    \"LightGBM\": LGBMRegressor(),\n    \"CatBoost\": CatBoostRegressor()\n}\n\nbest_model = None\nbest_rmse = float('inf')\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    model.fit(X_train_scaled, y_train)\n    y_pred_train = model.predict(X_train_scaled)\n    y_pred_val = model.predict(X_val_scaled)\n    \n    val_rmse = mean_squared_error(y_val, y_pred_val, squared=False)\n    \n    print(f\"Validation RMSE: {val_rmse:.2f}\")\n    print()\n    \n    if val_rmse < best_rmse:\n        best_rmse = val_rmse\n        best_model = model\n\nprint(f\"Making predictions using the best model...\")\npredictions = best_model.predict(test_data_scaled)\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions})\nsubmission.to_csv(\"best_model_submission.csv\", index=False)\nprint(f\"Predictions saved for the best model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:54:02.503594Z","iopub.execute_input":"2024-05-05T03:54:02.503966Z","iopub.status.idle":"2024-05-05T04:01:30.773177Z","shell.execute_reply.started":"2024-05-05T03:54:02.503935Z","shell.execute_reply":"2024-05-05T04:01:30.772226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:05:18.997843Z","iopub.execute_input":"2024-05-05T04:05:18.998199Z","iopub.status.idle":"2024-05-05T04:05:30.849301Z","shell.execute_reply.started":"2024-05-05T04:05:18.998171Z","shell.execute_reply":"2024-05-05T04:05:30.848272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\n\nmodel.compile(optimizer=Adam(), loss='mean_squared_error')\n\nhistory = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:05:30.851217Z","iopub.execute_input":"2024-05-05T04:05:30.852179Z","iopub.status.idle":"2024-05-05T04:10:04.304663Z","shell.execute_reply.started":"2024-05-05T04:05:30.852141Z","shell.execute_reply":"2024-05-05T04:10:04.303274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredictions = model.predict(test_data_scaled)\n\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions.flatten()})\nsubmission.to_csv(\"deep_learning_submission.csv\", index=False)\nprint(\"Predictions saved for the deep learning model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:10:39.213935Z","iopub.execute_input":"2024-05-05T04:10:39.214328Z","iopub.status.idle":"2024-05-05T04:11:22.862091Z","shell.execute_reply.started":"2024-05-05T04:10:39.214297Z","shell.execute_reply":"2024-05-05T04:11:22.861012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nparam_grid = {\n    'iterations': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [6, 8, 10],\n    'l2_leaf_reg': [1, 3, 5]\n}\n\n# Initialize CatBoostRegressor\ncatboost = CatBoostRegressor(verbose=0)\n\n# Perform GridSearchCV to find the best parameters\ngrid_search = GridSearchCV(estimator=catboost, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=2)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters:\", best_params)\n\n# Initialize CatBoost with the best parameters\nbest_catboost = CatBoostRegressor(**best_params, verbose=0)\n\n# Train the model\nbest_catboost.fit(X_train, y_train)\n\n# Evaluate the model\ntrain_rmse = mean_squared_error(y_train, best_catboost.predict(X_train), squared=False)\nval_rmse = mean_squared_error(y_val, best_catboost.predict(X_val), squared=False)\nprint(f\"Train RMSE: {train_rmse:.2f}\")\nprint(f\"Validation RMSE: {val_rmse:.2f}\")\n\n# Make predictions on test data\npredictions = best_catboost.predict(test_data)\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions})\nsubmission.to_csv(\"best_catboost_submission.csv\", index=False)\nprint(\"Predictions saved for the best CatBoost model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:13:58.229252Z","iopub.execute_input":"2024-05-05T04:13:58.230015Z","iopub.status.idle":"2024-05-05T05:49:30.453194Z","shell.execute_reply.started":"2024-05-05T04:13:58.229978Z","shell.execute_reply":"2024-05-05T05:49:30.452211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:34.274955Z","iopub.execute_input":"2024-05-13T16:59:34.275613Z","iopub.status.idle":"2024-05-13T16:59:34.279982Z","shell.execute_reply.started":"2024-05-13T16:59:34.275581Z","shell.execute_reply":"2024-05-13T16:59:34.279043Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:34.633666Z","iopub.execute_input":"2024-05-13T16:59:34.634017Z","iopub.status.idle":"2024-05-13T16:59:36.168799Z","shell.execute_reply.started":"2024-05-13T16:59:34.633988Z","shell.execute_reply":"2024-05-13T16:59:36.167819Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   id  MonsoonIntensity  TopographyDrainage  RiverManagement  Deforestation  \\\n0   0                 5                   8                5              8   \n1   1                 6                   7                4              4   \n2   2                 6                   5                6              7   \n3   3                 3                   4                6              5   \n4   4                 5                   3                2              6   \n\n   Urbanization  ClimateChange  DamsQuality  Siltation  AgriculturalPractices  \\\n0             6              4            4          3                      3   \n1             8              8            3          5                      4   \n2             3              7            1          5                      4   \n3             4              8            4          7                      6   \n4             4              4            3          3                      3   \n\n   ...  DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0  ...                5                     3           3           5   \n1  ...                7                     2           0           3   \n2  ...                7                     3           7           5   \n3  ...                2                     4           7           4   \n4  ...                2                     2           6           6   \n\n   DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                            4                7            5   \n1                            5                3            3   \n2                            6                8            2   \n3                            4                6            5   \n4                            4                1            2   \n\n   InadequatePlanning  PoliticalFactors  FloodProbability  \n0                   7                 3             0.445  \n1                   4                 3             0.450  \n2                   3                 3             0.530  \n3                   7                 5             0.535  \n4                   3                 5             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5</td>\n      <td>8</td>\n      <td>5</td>\n      <td>8</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>7</td>\n      <td>3</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>6</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>8</td>\n      <td>8</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>3</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4</td>\n      <td>7</td>\n      <td>6</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>6</td>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.170443Z","iopub.execute_input":"2024-05-13T16:59:36.170766Z","iopub.status.idle":"2024-05-13T16:59:36.198981Z","shell.execute_reply.started":"2024-05-13T16:59:36.170739Z","shell.execute_reply":"2024-05-13T16:59:36.198117Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"id                                 0\nMonsoonIntensity                   0\nTopographyDrainage                 0\nRiverManagement                    0\nDeforestation                      0\nUrbanization                       0\nClimateChange                      0\nDamsQuality                        0\nSiltation                          0\nAgriculturalPractices              0\nEncroachments                      0\nIneffectiveDisasterPreparedness    0\nDrainageSystems                    0\nCoastalVulnerability               0\nLandslides                         0\nWatersheds                         0\nDeterioratingInfrastructure        0\nPopulationScore                    0\nWetlandLoss                        0\nInadequatePlanning                 0\nPoliticalFactors                   0\nFloodProbability                   0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.200329Z","iopub.execute_input":"2024-05-13T16:59:36.201164Z","iopub.status.idle":"2024-05-13T16:59:36.238971Z","shell.execute_reply.started":"2024-05-13T16:59:36.201126Z","shell.execute_reply":"2024-05-13T16:59:36.238016Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1117957 entries, 0 to 1117956\nData columns (total 22 columns):\n #   Column                           Non-Null Count    Dtype  \n---  ------                           --------------    -----  \n 0   id                               1117957 non-null  int64  \n 1   MonsoonIntensity                 1117957 non-null  int64  \n 2   TopographyDrainage               1117957 non-null  int64  \n 3   RiverManagement                  1117957 non-null  int64  \n 4   Deforestation                    1117957 non-null  int64  \n 5   Urbanization                     1117957 non-null  int64  \n 6   ClimateChange                    1117957 non-null  int64  \n 7   DamsQuality                      1117957 non-null  int64  \n 8   Siltation                        1117957 non-null  int64  \n 9   AgriculturalPractices            1117957 non-null  int64  \n 10  Encroachments                    1117957 non-null  int64  \n 11  IneffectiveDisasterPreparedness  1117957 non-null  int64  \n 12  DrainageSystems                  1117957 non-null  int64  \n 13  CoastalVulnerability             1117957 non-null  int64  \n 14  Landslides                       1117957 non-null  int64  \n 15  Watersheds                       1117957 non-null  int64  \n 16  DeterioratingInfrastructure      1117957 non-null  int64  \n 17  PopulationScore                  1117957 non-null  int64  \n 18  WetlandLoss                      1117957 non-null  int64  \n 19  InadequatePlanning               1117957 non-null  int64  \n 20  PoliticalFactors                 1117957 non-null  int64  \n 21  FloodProbability                 1117957 non-null  float64\ndtypes: float64(1), int64(21)\nmemory usage: 187.6 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in df.columns : \n    print(column)\n    print(df[column].unique())\n    print(\"---------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.241560Z","iopub.execute_input":"2024-05-13T16:59:36.242249Z","iopub.status.idle":"2024-05-13T16:59:36.415194Z","shell.execute_reply.started":"2024-05-13T16:59:36.242213Z","shell.execute_reply":"2024-05-13T16:59:36.414109Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"id\n[      0       1       2 ... 1117954 1117955 1117956]\n---------------------------------------------------------------\nMonsoonIntensity\n[ 5  6  3  8  4  7  9  2 10  1  0 11 12 13 15 14 16]\n---------------------------------------------------------------\nTopographyDrainage\n[ 8  7  5  4  3  6  2  1  9 10 12  0 11 14 13 16 15 17 18]\n---------------------------------------------------------------\nRiverManagement\n[ 5  4  6  2  1  8  3  0  9  7 10 11 12 15 13 14 16]\n---------------------------------------------------------------\nDeforestation\n[ 8  4  7  5  6  2  3  9  0 10  1 13 11 12 14 15 16 17]\n---------------------------------------------------------------\nUrbanization\n[ 6  8  3  4  2  5 10  7  9 11  1  0 12 13 16 14 15 17]\n---------------------------------------------------------------\nClimateChange\n[ 4  8  7  5  6  3  2  1  0 10  9 12 11 13 14 15 16 17]\n---------------------------------------------------------------\nDamsQuality\n[ 4  3  1  6  2  5  8  7  9 12 11 10  0 14 13 15 16]\n---------------------------------------------------------------\nSiltation\n[ 3  5  7  6  4 10  8  1  2  9  0 11 14 12 13 15 16]\n---------------------------------------------------------------\nAgriculturalPractices\n[ 3  4  6  7  5  8  2 10  9  1  0 12 11 14 13 16 15]\n---------------------------------------------------------------\nEncroachments\n[ 4  6  5  8  3  7  2  9  0  1 10 13 11 12 14 15 16 17 18]\n---------------------------------------------------------------\nIneffectiveDisasterPreparedness\n[ 2  9  6  5  3  4  8  1  7 10  0 11 12 14 15 13 16]\n---------------------------------------------------------------\nDrainageSystems\n[ 5  7  2  3  9  6  4  1  8 10  0 13 11 12 14 15 17 16]\n---------------------------------------------------------------\nCoastalVulnerability\n[ 3  2  4  5  6  8  9 12  1 10  7  0 11 13 15 14 16 17]\n---------------------------------------------------------------\nLandslides\n[ 3  0  7  6  5  2  4  9  8 10  1 11 12 13 14 15 16]\n---------------------------------------------------------------\nWatersheds\n[ 5  3  4  6  8  7  9  2  0 11 10  1 12 13 16 14 15]\n---------------------------------------------------------------\nDeterioratingInfrastructure\n[ 4  5  6  8  7 11  3  2  9 10  0  1 13 12 14 15 16 17]\n---------------------------------------------------------------\nPopulationScore\n[ 7  3  8  6  1  5  4  2 12  9 10  0 11 13 15 14 16 17 18]\n---------------------------------------------------------------\nWetlandLoss\n[ 5  3  2  8  4  6  7  9 10  1  0 13 11 12 14 16 15 17 18 19]\n---------------------------------------------------------------\nInadequatePlanning\n[ 7  4  3  5  8  6  9  2  1 10 12 11  0 14 13 15 16]\n---------------------------------------------------------------\nPoliticalFactors\n[ 3  5  2  6  0  4  8 11  9  7 10  1 12 15 13 14 16]\n---------------------------------------------------------------\nFloodProbability\n[0.445 0.45  0.53  0.535 0.415 0.44  0.46  0.595 0.505 0.455 0.515 0.48\n 0.47  0.51  0.485 0.43  0.525 0.56  0.555 0.49  0.405 0.5   0.59  0.675\n 0.55  0.57  0.54  0.475 0.495 0.4   0.465 0.425 0.52  0.6   0.575 0.365\n 0.565 0.435 0.545 0.395 0.41  0.38  0.34  0.585 0.42  0.58  0.635 0.66\n 0.64  0.375 0.63  0.35  0.385 0.615 0.65  0.61  0.605 0.715 0.655 0.645\n 0.62  0.625 0.37  0.39  0.36  0.69  0.67  0.685 0.345 0.32  0.68  0.7\n 0.355 0.33  0.665 0.695 0.71  0.325 0.725 0.705 0.335 0.285 0.315]\n---------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.416439Z","iopub.execute_input":"2024-05-13T16:59:36.416784Z","iopub.status.idle":"2024-05-13T16:59:37.362217Z","shell.execute_reply.started":"2024-05-13T16:59:36.416758Z","shell.execute_reply":"2024-05-13T16:59:37.361280Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"        id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  1117957                 4                   6                3   \n1  1117958                 4                   4                2   \n2  1117959                 1                   3                6   \n3  1117960                 2                   4                4   \n4  1117961                 6                   3                2   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0              5             6              7            8          7   \n1              9             5              5            4          7   \n2              5             7              2            4          6   \n3              6             4              5            4          3   \n4              4             6              4            5          5   \n\n   AgriculturalPractices  ...  IneffectiveDisasterPreparedness  \\\n0                      8  ...                                8   \n1                      5  ...                                2   \n2                      4  ...                                7   \n3                      4  ...                                7   \n4                      3  ...                                4   \n\n   DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0                5                     7           5           6   \n1                4                     7           4           5   \n2                9                     2           5           5   \n3                8                     4           6           7   \n4                3                     2           6           4   \n\n   DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                            3                6            4   \n1                            1                7            4   \n2                            2                3            6   \n3                            6                4            2   \n4                            6                8            4   \n\n   InadequatePlanning  PoliticalFactors  \n0                   4                 5  \n1                   4                 3  \n2                   8                 3  \n3                   4                 4  \n4                   5                 5  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>IneffectiveDisasterPreparedness</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1117957</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1117958</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1117959</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>9</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>6</td>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1117960</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6</td>\n      <td>7</td>\n      <td>6</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1117961</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>6</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\nfor column in df.columns : \n    if column !='FloodProbability':\n        df[column] = (df[column]-df[column].min())/(df[column].max()-df[column].min())","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.363428Z","iopub.execute_input":"2024-05-13T16:59:37.363764Z","iopub.status.idle":"2024-05-13T16:59:37.531717Z","shell.execute_reply.started":"2024-05-13T16:59:37.363738Z","shell.execute_reply":"2024-05-13T16:59:37.530783Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.533087Z","iopub.execute_input":"2024-05-13T16:59:37.533704Z","iopub.status.idle":"2024-05-13T16:59:37.564026Z","shell.execute_reply.started":"2024-05-13T16:59:37.533653Z","shell.execute_reply":"2024-05-13T16:59:37.563129Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.565457Z","iopub.execute_input":"2024-05-13T16:59:37.565825Z","iopub.status.idle":"2024-05-13T16:59:37.570441Z","shell.execute_reply.started":"2024-05-13T16:59:37.565791Z","shell.execute_reply":"2024-05-13T16:59:37.569496Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.573475Z","iopub.execute_input":"2024-05-13T16:59:37.574099Z","iopub.status.idle":"2024-05-13T16:59:37.601799Z","shell.execute_reply.started":"2024-05-13T16:59:37.574043Z","shell.execute_reply":"2024-05-13T16:59:37.600824Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.603362Z","iopub.execute_input":"2024-05-13T16:59:37.603667Z","iopub.status.idle":"2024-05-13T16:59:37.631235Z","shell.execute_reply.started":"2024-05-13T16:59:37.603640Z","shell.execute_reply":"2024-05-13T16:59:37.630323Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(columns=['FloodProbability'])\ny = df['FloodProbability']","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.641266Z","iopub.execute_input":"2024-05-13T16:59:37.641736Z","iopub.status.idle":"2024-05-13T16:59:37.751467Z","shell.execute_reply.started":"2024-05-13T16:59:37.641712Z","shell.execute_reply":"2024-05-13T16:59:37.750451Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:38.278295Z","iopub.execute_input":"2024-05-13T16:59:38.278915Z","iopub.status.idle":"2024-05-13T16:59:38.284723Z","shell.execute_reply.started":"2024-05-13T16:59:38.278886Z","shell.execute_reply":"2024-05-13T16:59:38.283812Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(1117957, 21)"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:38.879181Z","iopub.execute_input":"2024-05-13T16:59:38.879882Z","iopub.status.idle":"2024-05-13T16:59:38.885363Z","shell.execute_reply.started":"2024-05-13T16:59:38.879852Z","shell.execute_reply":"2024-05-13T16:59:38.884431Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(1117957, 22)"},"metadata":{}}]},{"cell_type":"code","source":"X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:00:12.452880Z","iopub.execute_input":"2024-05-13T17:00:12.453243Z","iopub.status.idle":"2024-05-13T17:00:12.845442Z","shell.execute_reply.started":"2024-05-13T17:00:12.453213Z","shell.execute_reply":"2024-05-13T17:00:12.844590Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"dt.fit(X_train , y_train)\ny_pred = dt.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:01:23.248228Z","iopub.execute_input":"2024-05-13T17:01:23.249065Z","iopub.status.idle":"2024-05-13T17:01:34.796317Z","shell.execute_reply.started":"2024-05-13T17:01:23.249034Z","shell.execute_reply":"2024-05-13T17:01:34.795443Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score , mean_squared_error\nprint(r2_score(y_test , y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:02:47.181272Z","iopub.execute_input":"2024-05-13T17:02:47.181650Z","iopub.status.idle":"2024-05-13T17:02:47.191099Z","shell.execute_reply.started":"2024-05-13T17:02:47.181620Z","shell.execute_reply":"2024-05-13T17:02:47.190128Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"0.033002851179616766\n","output_type":"stream"}]},{"cell_type":"code","source":"print(mean_squared_error(y_test , y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:09.208770Z","iopub.execute_input":"2024-05-13T17:03:09.209530Z","iopub.status.idle":"2024-05-13T17:03:09.216917Z","shell.execute_reply.started":"2024-05-13T17:03:09.209497Z","shell.execute_reply":"2024-05-13T17:03:09.215860Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"0.0025138540287666833\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:20.531621Z","iopub.execute_input":"2024-05-13T17:03:20.531944Z","iopub.status.idle":"2024-05-13T17:03:20.644835Z","shell.execute_reply.started":"2024-05-13T17:03:20.531919Z","shell.execute_reply":"2024-05-13T17:03:20.643837Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0       1117957                 4                   6                3   \n1       1117958                 4                   4                2   \n2       1117959                 1                   3                6   \n3       1117960                 2                   4                4   \n4       1117961                 6                   3                2   \n...         ...               ...                 ...              ...   \n745300  1863257                 5                   4                8   \n745301  1863258                 4                   4                2   \n745302  1863259                 5                   7                9   \n745303  1863260                 4                   7                6   \n745304  1863261                 4                   2                5   \n\n        Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0                   5             6              7            8          7   \n1                   9             5              5            4          7   \n2                   5             7              2            4          6   \n3                   6             4              5            4          3   \n4                   4             6              4            5          5   \n...               ...           ...            ...          ...        ...   \n745300              3             5              4            4          5   \n745301             12             4              3            4          3   \n745302              5             5              6            7          5   \n745303              3             5              2            3          8   \n745304              3             8              4            5          3   \n\n        AgriculturalPractices  ...  IneffectiveDisasterPreparedness  \\\n0                           8  ...                                8   \n1                           5  ...                                2   \n2                           4  ...                                7   \n3                           4  ...                                7   \n4                           3  ...                                4   \n...                       ...  ...                              ...   \n745300                      5  ...                                5   \n745301                      5  ...                                3   \n745302                      5  ...                                6   \n745303                      6  ...                                6   \n745304                      5  ...                                4   \n\n        DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0                     5                     7           5           6   \n1                     4                     7           4           5   \n2                     9                     2           5           5   \n3                     8                     4           6           7   \n4                     3                     2           6           4   \n...                 ...                   ...         ...         ...   \n745300                6                     1           3           5   \n745301                7                     4           4           3   \n745302               11                     3          11           4   \n745303                6                     8           6           2   \n745304                2                     6          10           4   \n\n        DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                                 3                6            4   \n1                                 1                7            4   \n2                                 2                3            6   \n3                                 6                4            2   \n4                                 6                8            4   \n...                             ...              ...          ...   \n745300                            6                4            4   \n745301                            5                5            3   \n745302                            5                9            5   \n745303                            3                8            7   \n745304                            3                9            8   \n\n        InadequatePlanning  PoliticalFactors  \n0                        4                 5  \n1                        4                 3  \n2                        8                 3  \n3                        4                 4  \n4                        5                 5  \n...                    ...               ...  \n745300                   6                 6  \n745301                   5                 4  \n745302                   5                 4  \n745303                   5                 5  \n745304                   6                 3  \n\n[745305 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>IneffectiveDisasterPreparedness</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1117957</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1117958</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1117959</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>9</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>6</td>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1117960</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6</td>\n      <td>7</td>\n      <td>6</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1117961</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>6</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>745300</th>\n      <td>1863257</td>\n      <td>5</td>\n      <td>4</td>\n      <td>8</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>745301</th>\n      <td>1863258</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>3</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>745302</th>\n      <td>1863259</td>\n      <td>5</td>\n      <td>7</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>6</td>\n      <td>11</td>\n      <td>3</td>\n      <td>11</td>\n      <td>4</td>\n      <td>5</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>745303</th>\n      <td>1863260</td>\n      <td>4</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8</td>\n      <td>6</td>\n      <td>...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>8</td>\n      <td>6</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8</td>\n      <td>7</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>745304</th>\n      <td>1863261</td>\n      <td>4</td>\n      <td>2</td>\n      <td>5</td>\n      <td>3</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>4</td>\n      <td>3</td>\n      <td>9</td>\n      <td>8</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>745305 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s4e5/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:54.663485Z","iopub.execute_input":"2024-05-13T17:03:54.663842Z","iopub.status.idle":"2024-05-13T17:03:54.957704Z","shell.execute_reply.started":"2024-05-13T17:03:54.663814Z","shell.execute_reply":"2024-05-13T17:03:54.956704Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ntest_data_scaled = scaler.transform(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:20:46.321062Z","iopub.execute_input":"2024-05-21T19:20:46.321405Z","iopub.status.idle":"2024-05-21T19:20:49.109107Z","shell.execute_reply.started":"2024-05-21T19:20:46.321381Z","shell.execute_reply":"2024-05-21T19:20:49.108109Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.metrics import r2_score\n\n# Initialize K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nlgbm_r2_scores = []\n\nfor train_index, val_index in kf.split(X_scaled):\n    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n    y_train_fold, y_val_fold = y[train_index], y[val_index]\n    \n    # Initialize and train LightGBM model\n    lgbm = LGBMRegressor()\n    lgbm.fit(X_train_fold, y_train_fold)\n    \n    # Predict on validation set\n    lgbm_val_predictions = lgbm.predict(X_val_fold)\n    lgbm_r2 = r2_score(y_val_fold, lgbm_val_predictions)\n    lgbm_r2_scores.append(lgbm_r2)\n\nprint(f\"LightGBM Mean Validation R²: {np.mean(lgbm_r2_scores):.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:21:00.958980Z","iopub.execute_input":"2024-05-21T19:21:00.959811Z","iopub.status.idle":"2024-05-21T19:22:12.649083Z","shell.execute_reply.started":"2024-05-21T19:21:00.959780Z","shell.execute_reply":"2024-05-21T19:22:12.648050Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153427 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 623\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504511\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 623\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153131 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504482\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504471\nLightGBM Mean Validation R²: 0.77\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import r2_score\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ndl_r2_scores = []\n\nfor train_index, val_index in kf.split(X_scaled):\n    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n    y_train_fold, y_val_fold = y[train_index], y[val_index]\n    \n    # Build and compile the deep learning model\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dropout(0.2),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(), loss='mean_squared_error')\n\n    # Train the model\n    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=64, validation_data=(X_val_fold, y_val_fold), verbose=1)\n\n    # Predict on validation set\n    dl_val_predictions = model.predict(X_val_fold).flatten()\n    dl_r2 = r2_score(y_val_fold, dl_val_predictions)\n    dl_r2_scores.append(dl_r2)\n\nprint(f\"Deep Learning Mean Validation R²: {np.mean(dl_r2_scores):.2f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine predictions\ncombined_val_predictions = (lgbm_val_predictions + dl_val_predictions) / 2\ncombined_rmse = mean_squared_error(y_val, combined_val_predictions, squared=False)\nprint(f\"Combined Validation RMSE: {combined_rmse:.2f}\")\n\n# Train both models on the entire training data\nlgbm.fit(X, y)\nmodel.fit(scaler.transform(X), y, epochs=50, batch_size=32, verbose=1)\n\n# Make predictions on test data\nlgbm_test_predictions = lgbm.predict(test_data_scaled)\ndl_test_predictions = model.predict(test_data_scaled).flatten()\n\n# Combine test predictions\ncombined_test_predictions = (lgbm_test_predictions + dl_test_predictions) / 2\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': combined_test_predictions})\nsubmission.to_csv(\"combined_model_submission.csv\", index=False)\nprint(\"Predictions saved for the combined model\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:41:52.650095Z","iopub.execute_input":"2024-05-21T19:41:52.650469Z","iopub.status.idle":"2024-05-21T19:41:55.677642Z","shell.execute_reply.started":"2024-05-21T19:41:52.650440Z","shell.execute_reply":"2024-05-21T19:41:55.676765Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n# Initialize and train LightGBM model\nlgbm = LGBMRegressor()\nlgbm.fit(X_train_scaled, y_train)\n\n# Predict on validation set\nlgbm_val_predictions = lgbm.predict(X_val_scaled)\nlgbm_r2 = r2_score(y_val, lgbm_val_predictions)\nprint(f\"LightGBM Validation R²: {lgbm_r2:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:42:05.723284Z","iopub.execute_input":"2024-05-21T19:42:05.724169Z","iopub.status.idle":"2024-05-21T19:42:20.480810Z","shell.execute_reply.started":"2024-05-21T19:42:05.724135Z","shell.execute_reply":"2024-05-21T19:42:20.479842Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\nLightGBM Validation R²: 0.77\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Build and compile the deep learning model\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\nmodel.compile(optimizer=Adam(), loss='mean_squared_error')\n\n# Train the model\nmodel.fit(X_train_scaled, y_train, epochs=10, batch_size=64, validation_data=(X_val_scaled, y_val), verbose=1)\n\n# Predict on validation set\ndl_val_predictions = model.predict(X_val_scaled).flatten()\ndl_r2 = r2_score(y_val, dl_val_predictions)\nprint(f\"Deep Learning Validation R²: {dl_r2:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:42:35.390827Z","iopub.execute_input":"2024-05-21T19:42:35.391532Z","iopub.status.idle":"2024-05-21T19:46:56.686215Z","shell.execute_reply.started":"2024-05-21T19:42:35.391501Z","shell.execute_reply":"2024-05-21T19:46:56.685151Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 3.9320e-04\nEpoch 2/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.4248e-04 - val_loss: 3.8776e-04\nEpoch 3/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.2254e-04 - val_loss: 3.9074e-04\nEpoch 4/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.2037e-04 - val_loss: 4.4119e-04\nEpoch 5/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1738e-04 - val_loss: 3.8642e-04\nEpoch 6/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1644e-04 - val_loss: 4.4174e-04\nEpoch 7/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1481e-04 - val_loss: 4.1463e-04\nEpoch 8/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1463e-04 - val_loss: 3.7821e-04\nEpoch 9/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1369e-04 - val_loss: 4.0020e-04\nEpoch 10/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1335e-04 - val_loss: 4.1790e-04\n\u001b[1m6988/6988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\nDeep Learning Validation R²: 0.84\n","output_type":"stream"}]},{"cell_type":"code","source":"# Combine predictions\ncombined_val_predictions = (lgbm_val_predictions + dl_val_predictions) / 2\ncombined_r2 = r2_score(y_val, combined_val_predictions)\nprint(f\"Combined Validation R²: {combined_r2:.2f}\")\n\n# Train both models on the entire training data\nlgbm.fit(X, y)\nmodel.fit(scaler.transform(X), y, epochs=10, batch_size=32, verbose=1)\n\n# Make predictions on test data\nlgbm_test_predictions = lgbm.predict(test_data_scaled)\ndl_test_predictions = model.predict(test_data_scaled).flatten()\n\n# Combine test predictions\ncombined_test_predictions = (lgbm_test_predictions + dl_test_predictions) / 2\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': combined_test_predictions})\nsubmission.to_csv(\"combined_model_submission.csv\", index=False)\nprint(\"Predictions saved for the combined model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:51:01.298725Z","iopub.execute_input":"2024-05-21T19:51:01.299106Z","iopub.status.idle":"2024-05-21T20:00:26.114885Z","shell.execute_reply.started":"2024-05-21T19:51:01.299075Z","shell.execute_reply":"2024-05-21T20:00:26.113848Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Combined Validation R²: 0.82\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 1117957, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\nEpoch 1/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2517e-04\nEpoch 2/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2456e-04\nEpoch 3/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2446e-04\nEpoch 4/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2381e-04\nEpoch 5/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2336e-04\nEpoch 6/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1ms/step - loss: 4.2322e-04\nEpoch 7/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2248e-04\nEpoch 8/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.1930e-04\nEpoch 9/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1ms/step - loss: 4.2046e-04\nEpoch 10/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2037e-04\n\u001b[1m23291/23291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step\nPredictions saved for the combined model\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 50, 70],\n    'max_depth': [6, 8, 10],\n    'min_child_samples': [20, 30, 50],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9],\n    'reg_alpha': [0.0, 0.1, 1.0],\n    'reg_lambda': [0.0, 0.1, 1.0]\n}\n\n# Initialize the model\nlgbm = LGBMRegressor()\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters found: \", best_params)\n\nbest_lgbm = LGBMRegressor(**best_params)\n\n# Train the model\nbest_lgbm.fit(X_train_scaled, y_train)\n\n# Predict on validation set\nval_predictions = best_lgbm.predict(X_val_scaled)\nval_r2 = r2_score(y_val, val_predictions)\nprint(f\"Validation R²: {val_r2:.2f}\")\n\n# Make predictions on test data\ntest_predictions = best_lgbm.predict(test_data_scaled)\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': test_predictions})\nsubmission.to_csv(\"best_lgbm_submission.csv\", index=False)\nprint(\"Predictions saved for the best LightGBM model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T20:15:26.528735Z","iopub.execute_input":"2024-05-21T20:15:26.529111Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 19683 candidates, totalling 59049 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.336415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  33.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254496 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  39.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.315447 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  32.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.439177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  30.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282345 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  32.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  37.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.322585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  31.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246913 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  32.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  43.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  35.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269029 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  39.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280345 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  31.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  31.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  34.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  35.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.340283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  34.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346071 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  33.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.432622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  39.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306479 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  45.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263092 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  35.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  33.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.322198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  32.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  33.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.335912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  30.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295927 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  32.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  49.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  37.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291535 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  40.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  37.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  30.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300733 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  48.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.319126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  37.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  42.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  39.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266159 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  43.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  34.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  41.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277582 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  32.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296551 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  36.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  30.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287108 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  36.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  35.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299062 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  36.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  33.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  55.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283007 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  38.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  32.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  30.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256811 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  38.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.413854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  38.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  32.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296807 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  31.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  43.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273799 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  29.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  28.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.462197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  50.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309438 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time= 1.0min\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288683 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  39.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  38.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  39.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314496 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  36.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  42.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  39.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  35.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  34.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260491 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  31.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295903 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  35.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.349558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  35.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.279209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  35.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  32.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  45.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  32.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  37.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286953 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  35.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  51.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  39.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  34.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  39.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  38.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  36.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  55.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  32.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  36.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264707 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  34.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269816 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  34.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314826 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  33.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.278938 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  44.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  37.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272486 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  49.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296846 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  59.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262487 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  36.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.372800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  35.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248454 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  32.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  53.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  51.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.298261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  39.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289508 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  37.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304494 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  56.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  32.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time= 1.1min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272834 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  45.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  35.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  50.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  41.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.417453 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  32.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time= 1.1min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286758 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  44.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.255607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  34.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275826 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  42.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  55.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273421 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  33.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308394 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  45.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274123 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  46.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282264 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  34.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292860 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  33.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294546 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  34.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277525 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  32.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.310710 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  51.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263555 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  32.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290651 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  43.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  43.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  37.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312638 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  33.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302094 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  47.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.279241 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  38.3s\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  58.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  43.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.351113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  34.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242528 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  40.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  32.6s\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135107 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  37.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299294 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  41.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269838 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  34.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  30.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time= 1.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.339616 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  48.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  49.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308505 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  46.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  33.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292133 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  40.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  44.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  51.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291662 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  32.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  45.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  33.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  50.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.327411 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  41.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  36.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291753 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  50.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299983 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  36.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  34.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297906 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  50.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  44.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300365 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  31.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  46.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  50.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309078 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  33.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.331562 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  37.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296109 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  32.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  40.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  34.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207483 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  45.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.333073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  38.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  42.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  36.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  49.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  38.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252395 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  34.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292881 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  38.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260958 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  54.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277781 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  48.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time= 1.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  33.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  51.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261708 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  32.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262752 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  35.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283686 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  43.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  39.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287657 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time= 1.1min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  46.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  33.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.313277 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  39.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  47.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.369006 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  33.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  47.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  35.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299418 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  39.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time= 1.0min\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.279180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  37.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271129 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  34.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  47.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320057 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  42.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  33.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.237830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  34.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289678 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  33.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  32.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  40.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300432 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  51.6s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  46.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=70, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  45.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288991 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\n\n# Load data\ndf_train = pd.read_csv(\"/kaggle/input/playground-series-s4e5/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s4e5/test.csv\")\nsample_sub = pd.read_csv(\"/kaggle/input/playground-series-s4e5/sample_submission.csv\")\n\nprint(\"Train:\", len(df_train))\n\n# Exploratory Data Analysis\nprint(df_train.info())\nprint(df_train.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T08:21:24.609021Z","iopub.execute_input":"2024-05-22T08:21:24.609642Z","iopub.status.idle":"2024-05-22T08:21:34.178080Z","shell.execute_reply.started":"2024-05-22T08:21:24.609608Z","shell.execute_reply":"2024-05-22T08:21:34.176934Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Train: 1117957\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1117957 entries, 0 to 1117956\nData columns (total 22 columns):\n #   Column                           Non-Null Count    Dtype  \n---  ------                           --------------    -----  \n 0   id                               1117957 non-null  int64  \n 1   MonsoonIntensity                 1117957 non-null  int64  \n 2   TopographyDrainage               1117957 non-null  int64  \n 3   RiverManagement                  1117957 non-null  int64  \n 4   Deforestation                    1117957 non-null  int64  \n 5   Urbanization                     1117957 non-null  int64  \n 6   ClimateChange                    1117957 non-null  int64  \n 7   DamsQuality                      1117957 non-null  int64  \n 8   Siltation                        1117957 non-null  int64  \n 9   AgriculturalPractices            1117957 non-null  int64  \n 10  Encroachments                    1117957 non-null  int64  \n 11  IneffectiveDisasterPreparedness  1117957 non-null  int64  \n 12  DrainageSystems                  1117957 non-null  int64  \n 13  CoastalVulnerability             1117957 non-null  int64  \n 14  Landslides                       1117957 non-null  int64  \n 15  Watersheds                       1117957 non-null  int64  \n 16  DeterioratingInfrastructure      1117957 non-null  int64  \n 17  PopulationScore                  1117957 non-null  int64  \n 18  WetlandLoss                      1117957 non-null  int64  \n 19  InadequatePlanning               1117957 non-null  int64  \n 20  PoliticalFactors                 1117957 non-null  int64  \n 21  FloodProbability                 1117957 non-null  float64\ndtypes: float64(1), int64(21)\nmemory usage: 187.6 MB\nNone\n                 id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\ncount  1.117957e+06      1.117957e+06        1.117957e+06     1.117957e+06   \nmean   5.589780e+05      4.921450e+00        4.926671e+00     4.955322e+00   \nstd    3.227265e+05      2.056387e+00        2.093879e+00     2.072186e+00   \nmin    0.000000e+00      0.000000e+00        0.000000e+00     0.000000e+00   \n25%    2.794890e+05      3.000000e+00        3.000000e+00     4.000000e+00   \n50%    5.589780e+05      5.000000e+00        5.000000e+00     5.000000e+00   \n75%    8.384670e+05      6.000000e+00        6.000000e+00     6.000000e+00   \nmax    1.117956e+06      1.600000e+01        1.800000e+01     1.600000e+01   \n\n       Deforestation  Urbanization  ClimateChange   DamsQuality     Siltation  \\\ncount   1.117957e+06  1.117957e+06   1.117957e+06  1.117957e+06  1.117957e+06   \nmean    4.942240e+00  4.942517e+00   4.934093e+00  4.955878e+00  4.927791e+00   \nstd     2.051689e+00  2.083391e+00   2.057742e+00  2.083063e+00  2.065992e+00   \nmin     0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n25%     4.000000e+00  3.000000e+00   3.000000e+00  4.000000e+00  3.000000e+00   \n50%     5.000000e+00  5.000000e+00   5.000000e+00  5.000000e+00  5.000000e+00   \n75%     6.000000e+00  6.000000e+00   6.000000e+00  6.000000e+00  6.000000e+00   \nmax     1.700000e+01  1.700000e+01   1.700000e+01  1.600000e+01  1.600000e+01   \n\n       AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\ncount           1.117957e+06  ...     1.117957e+06          1.117957e+06   \nmean            4.942619e+00  ...     4.946893e+00          4.953999e+00   \nstd             2.068545e+00  ...     2.072333e+00          2.088899e+00   \nmin             0.000000e+00  ...     0.000000e+00          0.000000e+00   \n25%             3.000000e+00  ...     4.000000e+00          3.000000e+00   \n50%             5.000000e+00  ...     5.000000e+00          5.000000e+00   \n75%             6.000000e+00  ...     6.000000e+00          6.000000e+00   \nmax             1.600000e+01  ...     1.700000e+01          1.700000e+01   \n\n         Landslides    Watersheds  DeterioratingInfrastructure  \\\ncount  1.117957e+06  1.117957e+06                 1.117957e+06   \nmean   4.931376e+00  4.929032e+00                 4.925907e+00   \nstd    2.078287e+00  2.082395e+00                 2.064813e+00   \nmin    0.000000e+00  0.000000e+00                 0.000000e+00   \n25%    3.000000e+00  3.000000e+00                 3.000000e+00   \n50%    5.000000e+00  5.000000e+00                 5.000000e+00   \n75%    6.000000e+00  6.000000e+00                 6.000000e+00   \nmax    1.600000e+01  1.600000e+01                 1.700000e+01   \n\n       PopulationScore   WetlandLoss  InadequatePlanning  PoliticalFactors  \\\ncount     1.117957e+06  1.117957e+06        1.117957e+06      1.117957e+06   \nmean      4.927520e+00  4.950859e+00        4.940587e+00      4.939004e+00   \nstd       2.074176e+00  2.068696e+00        2.081123e+00      2.090350e+00   \nmin       0.000000e+00  0.000000e+00        0.000000e+00      0.000000e+00   \n25%       3.000000e+00  4.000000e+00        3.000000e+00      3.000000e+00   \n50%       5.000000e+00  5.000000e+00        5.000000e+00      5.000000e+00   \n75%       6.000000e+00  6.000000e+00        6.000000e+00      6.000000e+00   \nmax       1.800000e+01  1.900000e+01        1.600000e+01      1.600000e+01   \n\n       FloodProbability  \ncount      1.117957e+06  \nmean       5.044803e-01  \nstd        5.102610e-02  \nmin        2.850000e-01  \n25%        4.700000e-01  \n50%        5.050000e-01  \n75%        5.400000e-01  \nmax        7.250000e-01  \n\n[8 rows x 22 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"num_cols = [\n    'MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n    'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n    'Siltation', 'AgriculturalPractices', 'Encroachments',\n    'IneffectiveDisasterPreparedness', 'DrainageSystems',\n    'CoastalVulnerability', 'Landslides', 'Watersheds',\n    'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n    'InadequatePlanning', 'PoliticalFactors'\n]\ncorr_matrix = df_train[num_cols + ['FloodProbability']].corr()\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr_matrix, vmax=1, vmin=0.5, center=0.75, annot=True, fmt=\".2f\", square=True, \n            linewidths=.5, cmap='coolwarm')\nplt.title('Correlation Matrix of Numerical Features', fontsize=15)\nplt.xticks(fontsize=8, fontweight='bold')\nplt.yticks(fontsize=8, fontweight='bold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T08:22:07.094392Z","iopub.execute_input":"2024-05-22T08:22:07.094741Z","iopub.status.idle":"2024-05-22T08:22:09.843134Z","shell.execute_reply.started":"2024-05-22T08:22:07.094714Z","shell.execute_reply":"2024-05-22T08:22:09.841915Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1400x1200 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"unique_vals = []\nfor df in [df_train, df_test]:\n    for col in num_cols:\n        unique_vals += list(df[col].unique())\nunique_vals = list(set(unique_vals))\n\ndef getFeats(df):\n    scaler = StandardScaler()\n    \n    df['sum'] = df[num_cols].sum(axis=1)\n    df['std'] = df[num_cols].std(axis=1)\n    df['mean'] = df[num_cols].mean(axis=1)\n    df['max'] = df[num_cols].max(axis=1)\n    df['min'] = df[num_cols].min(axis=1)\n    df['mode'] = df[num_cols].mode(axis=1)[0]\n    df['median'] = df[num_cols].median(axis=1)\n    df['q_25th'] = df[num_cols].quantile(0.25, axis=1)\n    df['q_75th'] = df[num_cols].quantile(0.75, axis=1)\n    df['skew'] = df[num_cols].skew(axis=1)\n    df['kurt'] = df[num_cols].kurt(axis=1)\n    df['sum_72_76'] = df['sum'].isin(np.arange(72, 76))\n    for i in range(10, 100, 10):\n        df[f'{i}th'] = df[num_cols].quantile(i / 100, axis=1)\n    df['harmonic'] = len(num_cols) / df[num_cols].apply(lambda x: (1 / x).mean(), axis=1)\n    df['geometric'] = df[num_cols].apply(lambda x: x.prod()**(1 / len(x)), axis=1)\n    df['zscore'] = df[num_cols].apply(lambda x: (x - x.mean()) / x.std(), axis=1).mean(axis=1)\n    df['cv'] = df[num_cols].std(axis=1) / df[num_cols].mean(axis=1)\n    df['Skewness_75'] = (df[num_cols].quantile(0.75, axis=1) - df[num_cols].mean(axis=1)) / df[num_cols].std(axis=1)\n    df['Skewness_25'] = (df[num_cols].quantile(0.25, axis=1) - df[num_cols].mean(axis=1)) / df[num_cols].std(axis=1)\n    df['2ndMoment'] = df[num_cols].apply(lambda x: (x**2).mean(), axis=1)\n    df['3rdMoment'] = df[num_cols].apply(lambda x: (x**3).mean(), axis=1)\n    df['entropy'] = df[num_cols].apply(lambda x: -1 * (x * np.log(x)).sum(), axis=1)\n    \n    for v in unique_vals:\n        df['cnt_{}'.format(v)] = (df[num_cols] == v).sum(axis=1)\n    \n    df[num_cols] = scaler.fit_transform(df[num_cols])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-22T08:22:29.994653Z","iopub.execute_input":"2024-05-22T08:22:29.995022Z","iopub.status.idle":"2024-05-22T08:22:30.194642Z","shell.execute_reply.started":"2024-05-22T08:22:29.994992Z","shell.execute_reply":"2024-05-22T08:22:30.193572Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train['typ'] = 0\ndf_test['typ'] = 1\ndf_all = pd.concat([df_train, df_test], axis=0)\ndf_all = getFeats(df_all)\ndf_all.head()\ndf_train = df_all[df_all['typ'] == 0]\ndf_test = df_all[df_all['typ'] == 1]\n\nX = df_train.drop(['id', 'FloodProbability', 'typ'], axis=1)\ny = df_train['FloodProbability']\nfeats = list(X.columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T08:22:40.339258Z","iopub.execute_input":"2024-05-22T08:22:40.339617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_val_train(X, y, df_test):\n    spl = 5\n    test_preds = np.zeros((len(df_test)))\n    val_preds = np.zeros((len(X)))\n    val_scores, train_scores = [], []\n    \n    cv = KFold(spl, shuffle=True, random_state=42)\n    \n    for fold, (train_ind, valid_ind) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_ind]\n        y_train = y[train_ind]\n        X_val = X.iloc[valid_ind]\n        y_val = y[valid_ind]\n        \n        model = lgb.LGBMRegressor()\n        model.fit(X_train, y_train,\n                  eval_set=[(X_val, y_val)],\n                  callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)])\n        \n        y_pred_trn = model.predict(X_train)\n        y_pred_val = model.predict(X_val)\n        train_r2 = r2_score(y_train, y_pred_trn)\n        val_r2 = r2_score(y_val, y_pred_val)\n        print(\"Fold:\", fold, \" Train R2:\", np.round(train_r2, 5), \" Val R2:\", np.round(val_r2, 5))\n        \n        test_preds += model.predict(df_test[feats]) / spl\n        val_preds[valid_ind] = model.predict(X_val)\n        val_scores.append(val_r2)\n        print(\"-\" * 50)\n        \n    return val_scores, val_preds, test_preds\n\nval_scores, val_preds, test_preds = cross_val_train(X, y, df_test)\n\n# Evaluate the model\nmse = mean_squared_error(y, val_preds)\nrmse = np.sqrt(mean_squared_error(y, val_preds))\nr2 = r2_score(y, val_preds)\n\nprint(f'MSE: {mse}')\nprint(f'RMSE: {rmse}')\nprint(f'R2: {r2}')\n\n# Plot Residuals\n\n\n# Submission\nsub = sample_sub[['id']]\nsub['FloodProbability'] = test_preds\nsub.to_csv('submission.csv', index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}