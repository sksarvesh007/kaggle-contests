{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:27.563372Z","iopub.execute_input":"2024-05-05T03:48:27.563986Z","iopub.status.idle":"2024-05-05T03:48:27.931796Z","shell.execute_reply.started":"2024-05-05T03:48:27.563954Z","shell.execute_reply":"2024-05-05T03:48:27.931014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:28.276414Z","iopub.execute_input":"2024-05-05T03:48:28.277253Z","iopub.status.idle":"2024-05-05T03:48:30.364502Z","shell.execute_reply.started":"2024-05-05T03:48:28.277219Z","shell.execute_reply":"2024-05-05T03:48:30.363706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:30.366119Z","iopub.execute_input":"2024-05-05T03:48:30.366442Z","iopub.status.idle":"2024-05-05T03:48:34.689183Z","shell.execute_reply.started":"2024-05-05T03:48:30.366416Z","shell.execute_reply":"2024-05-05T03:48:34.688384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/playground-series-s4e5/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:48:34.690403Z","iopub.execute_input":"2024-05-05T03:48:34.690791Z","iopub.status.idle":"2024-05-05T03:48:35.981225Z","shell.execute_reply.started":"2024-05-05T03:48:34.690749Z","shell.execute_reply":"2024-05-05T03:48:35.980418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(),\n    \"Gradient Boosting\": GradientBoostingRegressor(),\n    \"Support Vector Machine\": SVR(),\n    \"XGBoost\": XGBRegressor(),\n    \"LightGBM\": LGBMRegressor(),\n    \"CatBoost\": CatBoostRegressor()\n}\n\nbest_model = None\nbest_rmse = float('inf')\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    model.fit(X_train_scaled, y_train)\n    y_pred_train = model.predict(X_train_scaled)\n    y_pred_val = model.predict(X_val_scaled)\n    \n    val_rmse = mean_squared_error(y_val, y_pred_val, squared=False)\n    \n    print(f\"Validation RMSE: {val_rmse:.2f}\")\n    print()\n    \n    if val_rmse < best_rmse:\n        best_rmse = val_rmse\n        best_model = model\n\nprint(f\"Making predictions using the best model...\")\npredictions = best_model.predict(test_data_scaled)\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions})\nsubmission.to_csv(\"best_model_submission.csv\", index=False)\nprint(f\"Predictions saved for the best model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T03:54:02.503594Z","iopub.execute_input":"2024-05-05T03:54:02.503966Z","iopub.status.idle":"2024-05-05T04:01:30.773177Z","shell.execute_reply.started":"2024-05-05T03:54:02.503935Z","shell.execute_reply":"2024-05-05T04:01:30.772226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:05:18.997843Z","iopub.execute_input":"2024-05-05T04:05:18.998199Z","iopub.status.idle":"2024-05-05T04:05:30.849301Z","shell.execute_reply.started":"2024-05-05T04:05:18.998171Z","shell.execute_reply":"2024-05-05T04:05:30.848272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\n\nmodel.compile(optimizer=Adam(), loss='mean_squared_error')\n\nhistory = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:05:30.851217Z","iopub.execute_input":"2024-05-05T04:05:30.852179Z","iopub.status.idle":"2024-05-05T04:10:04.304663Z","shell.execute_reply.started":"2024-05-05T04:05:30.852141Z","shell.execute_reply":"2024-05-05T04:10:04.303274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredictions = model.predict(test_data_scaled)\n\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions.flatten()})\nsubmission.to_csv(\"deep_learning_submission.csv\", index=False)\nprint(\"Predictions saved for the deep learning model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:10:39.213935Z","iopub.execute_input":"2024-05-05T04:10:39.214328Z","iopub.status.idle":"2024-05-05T04:11:22.862091Z","shell.execute_reply.started":"2024-05-05T04:10:39.214297Z","shell.execute_reply":"2024-05-05T04:11:22.861012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nparam_grid = {\n    'iterations': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [6, 8, 10],\n    'l2_leaf_reg': [1, 3, 5]\n}\n\n# Initialize CatBoostRegressor\ncatboost = CatBoostRegressor(verbose=0)\n\n# Perform GridSearchCV to find the best parameters\ngrid_search = GridSearchCV(estimator=catboost, param_grid=param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=2)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters:\", best_params)\n\n# Initialize CatBoost with the best parameters\nbest_catboost = CatBoostRegressor(**best_params, verbose=0)\n\n# Train the model\nbest_catboost.fit(X_train, y_train)\n\n# Evaluate the model\ntrain_rmse = mean_squared_error(y_train, best_catboost.predict(X_train), squared=False)\nval_rmse = mean_squared_error(y_val, best_catboost.predict(X_val), squared=False)\nprint(f\"Train RMSE: {train_rmse:.2f}\")\nprint(f\"Validation RMSE: {val_rmse:.2f}\")\n\n# Make predictions on test data\npredictions = best_catboost.predict(test_data)\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': predictions})\nsubmission.to_csv(\"best_catboost_submission.csv\", index=False)\nprint(\"Predictions saved for the best CatBoost model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T04:13:58.229252Z","iopub.execute_input":"2024-05-05T04:13:58.230015Z","iopub.status.idle":"2024-05-05T05:49:30.453194Z","shell.execute_reply.started":"2024-05-05T04:13:58.229978Z","shell.execute_reply":"2024-05-05T05:49:30.452211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:34.274955Z","iopub.execute_input":"2024-05-13T16:59:34.275613Z","iopub.status.idle":"2024-05-13T16:59:34.279982Z","shell.execute_reply.started":"2024-05-13T16:59:34.275581Z","shell.execute_reply":"2024-05-13T16:59:34.279043Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:34.633666Z","iopub.execute_input":"2024-05-13T16:59:34.634017Z","iopub.status.idle":"2024-05-13T16:59:36.168799Z","shell.execute_reply.started":"2024-05-13T16:59:34.633988Z","shell.execute_reply":"2024-05-13T16:59:36.167819Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"   id  MonsoonIntensity  TopographyDrainage  RiverManagement  Deforestation  \\\n0   0                 5                   8                5              8   \n1   1                 6                   7                4              4   \n2   2                 6                   5                6              7   \n3   3                 3                   4                6              5   \n4   4                 5                   3                2              6   \n\n   Urbanization  ClimateChange  DamsQuality  Siltation  AgriculturalPractices  \\\n0             6              4            4          3                      3   \n1             8              8            3          5                      4   \n2             3              7            1          5                      4   \n3             4              8            4          7                      6   \n4             4              4            3          3                      3   \n\n   ...  DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0  ...                5                     3           3           5   \n1  ...                7                     2           0           3   \n2  ...                7                     3           7           5   \n3  ...                2                     4           7           4   \n4  ...                2                     2           6           6   \n\n   DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                            4                7            5   \n1                            5                3            3   \n2                            6                8            2   \n3                            4                6            5   \n4                            4                1            2   \n\n   InadequatePlanning  PoliticalFactors  FloodProbability  \n0                   7                 3             0.445  \n1                   4                 3             0.450  \n2                   3                 3             0.530  \n3                   7                 5             0.535  \n4                   3                 5             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>5</td>\n      <td>8</td>\n      <td>5</td>\n      <td>8</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>7</td>\n      <td>3</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>6</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>8</td>\n      <td>8</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>6</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>5</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>3</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>8</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>3</td>\n      <td>4</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4</td>\n      <td>7</td>\n      <td>6</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>3</td>\n      <td>3</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>6</td>\n      <td>6</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>5</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.170443Z","iopub.execute_input":"2024-05-13T16:59:36.170766Z","iopub.status.idle":"2024-05-13T16:59:36.198981Z","shell.execute_reply.started":"2024-05-13T16:59:36.170739Z","shell.execute_reply":"2024-05-13T16:59:36.198117Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"id                                 0\nMonsoonIntensity                   0\nTopographyDrainage                 0\nRiverManagement                    0\nDeforestation                      0\nUrbanization                       0\nClimateChange                      0\nDamsQuality                        0\nSiltation                          0\nAgriculturalPractices              0\nEncroachments                      0\nIneffectiveDisasterPreparedness    0\nDrainageSystems                    0\nCoastalVulnerability               0\nLandslides                         0\nWatersheds                         0\nDeterioratingInfrastructure        0\nPopulationScore                    0\nWetlandLoss                        0\nInadequatePlanning                 0\nPoliticalFactors                   0\nFloodProbability                   0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.200329Z","iopub.execute_input":"2024-05-13T16:59:36.201164Z","iopub.status.idle":"2024-05-13T16:59:36.238971Z","shell.execute_reply.started":"2024-05-13T16:59:36.201126Z","shell.execute_reply":"2024-05-13T16:59:36.238016Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1117957 entries, 0 to 1117956\nData columns (total 22 columns):\n #   Column                           Non-Null Count    Dtype  \n---  ------                           --------------    -----  \n 0   id                               1117957 non-null  int64  \n 1   MonsoonIntensity                 1117957 non-null  int64  \n 2   TopographyDrainage               1117957 non-null  int64  \n 3   RiverManagement                  1117957 non-null  int64  \n 4   Deforestation                    1117957 non-null  int64  \n 5   Urbanization                     1117957 non-null  int64  \n 6   ClimateChange                    1117957 non-null  int64  \n 7   DamsQuality                      1117957 non-null  int64  \n 8   Siltation                        1117957 non-null  int64  \n 9   AgriculturalPractices            1117957 non-null  int64  \n 10  Encroachments                    1117957 non-null  int64  \n 11  IneffectiveDisasterPreparedness  1117957 non-null  int64  \n 12  DrainageSystems                  1117957 non-null  int64  \n 13  CoastalVulnerability             1117957 non-null  int64  \n 14  Landslides                       1117957 non-null  int64  \n 15  Watersheds                       1117957 non-null  int64  \n 16  DeterioratingInfrastructure      1117957 non-null  int64  \n 17  PopulationScore                  1117957 non-null  int64  \n 18  WetlandLoss                      1117957 non-null  int64  \n 19  InadequatePlanning               1117957 non-null  int64  \n 20  PoliticalFactors                 1117957 non-null  int64  \n 21  FloodProbability                 1117957 non-null  float64\ndtypes: float64(1), int64(21)\nmemory usage: 187.6 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"for column in df.columns : \n    print(column)\n    print(df[column].unique())\n    print(\"---------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.241560Z","iopub.execute_input":"2024-05-13T16:59:36.242249Z","iopub.status.idle":"2024-05-13T16:59:36.415194Z","shell.execute_reply.started":"2024-05-13T16:59:36.242213Z","shell.execute_reply":"2024-05-13T16:59:36.414109Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"id\n[      0       1       2 ... 1117954 1117955 1117956]\n---------------------------------------------------------------\nMonsoonIntensity\n[ 5  6  3  8  4  7  9  2 10  1  0 11 12 13 15 14 16]\n---------------------------------------------------------------\nTopographyDrainage\n[ 8  7  5  4  3  6  2  1  9 10 12  0 11 14 13 16 15 17 18]\n---------------------------------------------------------------\nRiverManagement\n[ 5  4  6  2  1  8  3  0  9  7 10 11 12 15 13 14 16]\n---------------------------------------------------------------\nDeforestation\n[ 8  4  7  5  6  2  3  9  0 10  1 13 11 12 14 15 16 17]\n---------------------------------------------------------------\nUrbanization\n[ 6  8  3  4  2  5 10  7  9 11  1  0 12 13 16 14 15 17]\n---------------------------------------------------------------\nClimateChange\n[ 4  8  7  5  6  3  2  1  0 10  9 12 11 13 14 15 16 17]\n---------------------------------------------------------------\nDamsQuality\n[ 4  3  1  6  2  5  8  7  9 12 11 10  0 14 13 15 16]\n---------------------------------------------------------------\nSiltation\n[ 3  5  7  6  4 10  8  1  2  9  0 11 14 12 13 15 16]\n---------------------------------------------------------------\nAgriculturalPractices\n[ 3  4  6  7  5  8  2 10  9  1  0 12 11 14 13 16 15]\n---------------------------------------------------------------\nEncroachments\n[ 4  6  5  8  3  7  2  9  0  1 10 13 11 12 14 15 16 17 18]\n---------------------------------------------------------------\nIneffectiveDisasterPreparedness\n[ 2  9  6  5  3  4  8  1  7 10  0 11 12 14 15 13 16]\n---------------------------------------------------------------\nDrainageSystems\n[ 5  7  2  3  9  6  4  1  8 10  0 13 11 12 14 15 17 16]\n---------------------------------------------------------------\nCoastalVulnerability\n[ 3  2  4  5  6  8  9 12  1 10  7  0 11 13 15 14 16 17]\n---------------------------------------------------------------\nLandslides\n[ 3  0  7  6  5  2  4  9  8 10  1 11 12 13 14 15 16]\n---------------------------------------------------------------\nWatersheds\n[ 5  3  4  6  8  7  9  2  0 11 10  1 12 13 16 14 15]\n---------------------------------------------------------------\nDeterioratingInfrastructure\n[ 4  5  6  8  7 11  3  2  9 10  0  1 13 12 14 15 16 17]\n---------------------------------------------------------------\nPopulationScore\n[ 7  3  8  6  1  5  4  2 12  9 10  0 11 13 15 14 16 17 18]\n---------------------------------------------------------------\nWetlandLoss\n[ 5  3  2  8  4  6  7  9 10  1  0 13 11 12 14 16 15 17 18 19]\n---------------------------------------------------------------\nInadequatePlanning\n[ 7  4  3  5  8  6  9  2  1 10 12 11  0 14 13 15 16]\n---------------------------------------------------------------\nPoliticalFactors\n[ 3  5  2  6  0  4  8 11  9  7 10  1 12 15 13 14 16]\n---------------------------------------------------------------\nFloodProbability\n[0.445 0.45  0.53  0.535 0.415 0.44  0.46  0.595 0.505 0.455 0.515 0.48\n 0.47  0.51  0.485 0.43  0.525 0.56  0.555 0.49  0.405 0.5   0.59  0.675\n 0.55  0.57  0.54  0.475 0.495 0.4   0.465 0.425 0.52  0.6   0.575 0.365\n 0.565 0.435 0.545 0.395 0.41  0.38  0.34  0.585 0.42  0.58  0.635 0.66\n 0.64  0.375 0.63  0.35  0.385 0.615 0.65  0.61  0.605 0.715 0.655 0.645\n 0.62  0.625 0.37  0.39  0.36  0.69  0.67  0.685 0.345 0.32  0.68  0.7\n 0.355 0.33  0.665 0.695 0.71  0.325 0.725 0.705 0.335 0.285 0.315]\n---------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:36.416439Z","iopub.execute_input":"2024-05-13T16:59:36.416784Z","iopub.status.idle":"2024-05-13T16:59:37.362217Z","shell.execute_reply.started":"2024-05-13T16:59:36.416758Z","shell.execute_reply":"2024-05-13T16:59:37.361280Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"        id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  1117957                 4                   6                3   \n1  1117958                 4                   4                2   \n2  1117959                 1                   3                6   \n3  1117960                 2                   4                4   \n4  1117961                 6                   3                2   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0              5             6              7            8          7   \n1              9             5              5            4          7   \n2              5             7              2            4          6   \n3              6             4              5            4          3   \n4              4             6              4            5          5   \n\n   AgriculturalPractices  ...  IneffectiveDisasterPreparedness  \\\n0                      8  ...                                8   \n1                      5  ...                                2   \n2                      4  ...                                7   \n3                      4  ...                                7   \n4                      3  ...                                4   \n\n   DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0                5                     7           5           6   \n1                4                     7           4           5   \n2                9                     2           5           5   \n3                8                     4           6           7   \n4                3                     2           6           4   \n\n   DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                            3                6            4   \n1                            1                7            4   \n2                            2                3            6   \n3                            6                4            2   \n4                            6                8            4   \n\n   InadequatePlanning  PoliticalFactors  \n0                   4                 5  \n1                   4                 3  \n2                   8                 3  \n3                   4                 4  \n4                   5                 5  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>IneffectiveDisasterPreparedness</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1117957</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1117958</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1117959</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>9</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>6</td>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1117960</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6</td>\n      <td>7</td>\n      <td>6</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1117961</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>6</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\nfor column in df.columns : \n    if column !='FloodProbability':\n        df[column] = (df[column]-df[column].min())/(df[column].max()-df[column].min())","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.363428Z","iopub.execute_input":"2024-05-13T16:59:37.363764Z","iopub.status.idle":"2024-05-13T16:59:37.531717Z","shell.execute_reply.started":"2024-05-13T16:59:37.363738Z","shell.execute_reply":"2024-05-13T16:59:37.530783Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.533087Z","iopub.execute_input":"2024-05-13T16:59:37.533704Z","iopub.status.idle":"2024-05-13T16:59:37.564026Z","shell.execute_reply.started":"2024-05-13T16:59:37.533653Z","shell.execute_reply":"2024-05-13T16:59:37.563129Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.565457Z","iopub.execute_input":"2024-05-13T16:59:37.565825Z","iopub.status.idle":"2024-05-13T16:59:37.570441Z","shell.execute_reply.started":"2024-05-13T16:59:37.565791Z","shell.execute_reply":"2024-05-13T16:59:37.569496Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.573475Z","iopub.execute_input":"2024-05-13T16:59:37.574099Z","iopub.status.idle":"2024-05-13T16:59:37.601799Z","shell.execute_reply.started":"2024-05-13T16:59:37.574043Z","shell.execute_reply":"2024-05-13T16:59:37.600824Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.603362Z","iopub.execute_input":"2024-05-13T16:59:37.603667Z","iopub.status.idle":"2024-05-13T16:59:37.631235Z","shell.execute_reply.started":"2024-05-13T16:59:37.603640Z","shell.execute_reply":"2024-05-13T16:59:37.630323Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0  0.000000e+00            0.3125            0.444444           0.3125   \n1  8.944896e-07            0.3750            0.388889           0.2500   \n2  1.788979e-06            0.3750            0.277778           0.3750   \n3  2.683469e-06            0.1875            0.222222           0.3750   \n4  3.577958e-06            0.3125            0.166667           0.1250   \n\n   Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0       0.470588      0.352941       0.235294       0.2500     0.1875   \n1       0.235294      0.470588       0.470588       0.1875     0.3125   \n2       0.411765      0.176471       0.411765       0.0625     0.3125   \n3       0.294118      0.235294       0.470588       0.2500     0.4375   \n4       0.352941      0.235294       0.235294       0.1875     0.1875   \n\n   AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n0                 0.1875  ...         0.294118              0.176471   \n1                 0.2500  ...         0.411765              0.117647   \n2                 0.2500  ...         0.411765              0.176471   \n3                 0.3750  ...         0.117647              0.235294   \n4                 0.1875  ...         0.117647              0.117647   \n\n   Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n0      0.1875      0.3125                     0.235294         0.388889   \n1      0.0000      0.1875                     0.294118         0.166667   \n2      0.4375      0.3125                     0.352941         0.444444   \n3      0.4375      0.2500                     0.235294         0.333333   \n4      0.3750      0.3750                     0.235294         0.055556   \n\n   WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n0     0.263158              0.4375            0.1875             0.445  \n1     0.157895              0.2500            0.1875             0.450  \n2     0.105263              0.1875            0.1875             0.530  \n3     0.263158              0.4375            0.3125             0.535  \n4     0.105263              0.1875            0.3125             0.415  \n\n[5 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n      <th>FloodProbability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000e+00</td>\n      <td>0.3125</td>\n      <td>0.444444</td>\n      <td>0.3125</td>\n      <td>0.470588</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.294118</td>\n      <td>0.176471</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.235294</td>\n      <td>0.388889</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.1875</td>\n      <td>0.445</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.944896e-07</td>\n      <td>0.3750</td>\n      <td>0.388889</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.470588</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>0.0000</td>\n      <td>0.1875</td>\n      <td>0.294118</td>\n      <td>0.166667</td>\n      <td>0.157895</td>\n      <td>0.2500</td>\n      <td>0.1875</td>\n      <td>0.450</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.788979e-06</td>\n      <td>0.3750</td>\n      <td>0.277778</td>\n      <td>0.3750</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.411765</td>\n      <td>0.0625</td>\n      <td>0.3125</td>\n      <td>0.2500</td>\n      <td>...</td>\n      <td>0.411765</td>\n      <td>0.176471</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.352941</td>\n      <td>0.444444</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.683469e-06</td>\n      <td>0.1875</td>\n      <td>0.222222</td>\n      <td>0.3750</td>\n      <td>0.294118</td>\n      <td>0.235294</td>\n      <td>0.470588</td>\n      <td>0.2500</td>\n      <td>0.4375</td>\n      <td>0.3750</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.235294</td>\n      <td>0.4375</td>\n      <td>0.2500</td>\n      <td>0.235294</td>\n      <td>0.333333</td>\n      <td>0.263158</td>\n      <td>0.4375</td>\n      <td>0.3125</td>\n      <td>0.535</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.577958e-06</td>\n      <td>0.3125</td>\n      <td>0.166667</td>\n      <td>0.1250</td>\n      <td>0.352941</td>\n      <td>0.235294</td>\n      <td>0.235294</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>0.1875</td>\n      <td>...</td>\n      <td>0.117647</td>\n      <td>0.117647</td>\n      <td>0.3750</td>\n      <td>0.3750</td>\n      <td>0.235294</td>\n      <td>0.055556</td>\n      <td>0.105263</td>\n      <td>0.1875</td>\n      <td>0.3125</td>\n      <td>0.415</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(columns=['FloodProbability'])\ny = df['FloodProbability']","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:37.641266Z","iopub.execute_input":"2024-05-13T16:59:37.641736Z","iopub.status.idle":"2024-05-13T16:59:37.751467Z","shell.execute_reply.started":"2024-05-13T16:59:37.641712Z","shell.execute_reply":"2024-05-13T16:59:37.750451Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:38.278295Z","iopub.execute_input":"2024-05-13T16:59:38.278915Z","iopub.status.idle":"2024-05-13T16:59:38.284723Z","shell.execute_reply.started":"2024-05-13T16:59:38.278886Z","shell.execute_reply":"2024-05-13T16:59:38.283812Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(1117957, 21)"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-13T16:59:38.879181Z","iopub.execute_input":"2024-05-13T16:59:38.879882Z","iopub.status.idle":"2024-05-13T16:59:38.885363Z","shell.execute_reply.started":"2024-05-13T16:59:38.879852Z","shell.execute_reply":"2024-05-13T16:59:38.884431Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"(1117957, 22)"},"metadata":{}}]},{"cell_type":"code","source":"X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:00:12.452880Z","iopub.execute_input":"2024-05-13T17:00:12.453243Z","iopub.status.idle":"2024-05-13T17:00:12.845442Z","shell.execute_reply.started":"2024-05-13T17:00:12.453213Z","shell.execute_reply":"2024-05-13T17:00:12.844590Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"dt.fit(X_train , y_train)\ny_pred = dt.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:01:23.248228Z","iopub.execute_input":"2024-05-13T17:01:23.249065Z","iopub.status.idle":"2024-05-13T17:01:34.796317Z","shell.execute_reply.started":"2024-05-13T17:01:23.249034Z","shell.execute_reply":"2024-05-13T17:01:34.795443Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import r2_score , mean_squared_error\nprint(r2_score(y_test , y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:02:47.181272Z","iopub.execute_input":"2024-05-13T17:02:47.181650Z","iopub.status.idle":"2024-05-13T17:02:47.191099Z","shell.execute_reply.started":"2024-05-13T17:02:47.181620Z","shell.execute_reply":"2024-05-13T17:02:47.190128Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"0.033002851179616766\n","output_type":"stream"}]},{"cell_type":"code","source":"print(mean_squared_error(y_test , y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:09.208770Z","iopub.execute_input":"2024-05-13T17:03:09.209530Z","iopub.status.idle":"2024-05-13T17:03:09.216917Z","shell.execute_reply.started":"2024-05-13T17:03:09.209497Z","shell.execute_reply":"2024-05-13T17:03:09.215860Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"0.0025138540287666833\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:20.531621Z","iopub.execute_input":"2024-05-13T17:03:20.531944Z","iopub.status.idle":"2024-05-13T17:03:20.644835Z","shell.execute_reply.started":"2024-05-13T17:03:20.531919Z","shell.execute_reply":"2024-05-13T17:03:20.643837Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"             id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n0       1117957                 4                   6                3   \n1       1117958                 4                   4                2   \n2       1117959                 1                   3                6   \n3       1117960                 2                   4                4   \n4       1117961                 6                   3                2   \n...         ...               ...                 ...              ...   \n745300  1863257                 5                   4                8   \n745301  1863258                 4                   4                2   \n745302  1863259                 5                   7                9   \n745303  1863260                 4                   7                6   \n745304  1863261                 4                   2                5   \n\n        Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n0                   5             6              7            8          7   \n1                   9             5              5            4          7   \n2                   5             7              2            4          6   \n3                   6             4              5            4          3   \n4                   4             6              4            5          5   \n...               ...           ...            ...          ...        ...   \n745300              3             5              4            4          5   \n745301             12             4              3            4          3   \n745302              5             5              6            7          5   \n745303              3             5              2            3          8   \n745304              3             8              4            5          3   \n\n        AgriculturalPractices  ...  IneffectiveDisasterPreparedness  \\\n0                           8  ...                                8   \n1                           5  ...                                2   \n2                           4  ...                                7   \n3                           4  ...                                7   \n4                           3  ...                                4   \n...                       ...  ...                              ...   \n745300                      5  ...                                5   \n745301                      5  ...                                3   \n745302                      5  ...                                6   \n745303                      6  ...                                6   \n745304                      5  ...                                4   \n\n        DrainageSystems  CoastalVulnerability  Landslides  Watersheds  \\\n0                     5                     7           5           6   \n1                     4                     7           4           5   \n2                     9                     2           5           5   \n3                     8                     4           6           7   \n4                     3                     2           6           4   \n...                 ...                   ...         ...         ...   \n745300                6                     1           3           5   \n745301                7                     4           4           3   \n745302               11                     3          11           4   \n745303                6                     8           6           2   \n745304                2                     6          10           4   \n\n        DeterioratingInfrastructure  PopulationScore  WetlandLoss  \\\n0                                 3                6            4   \n1                                 1                7            4   \n2                                 2                3            6   \n3                                 6                4            2   \n4                                 6                8            4   \n...                             ...              ...          ...   \n745300                            6                4            4   \n745301                            5                5            3   \n745302                            5                9            5   \n745303                            3                8            7   \n745304                            3                9            8   \n\n        InadequatePlanning  PoliticalFactors  \n0                        4                 5  \n1                        4                 3  \n2                        8                 3  \n3                        4                 4  \n4                        5                 5  \n...                    ...               ...  \n745300                   6                 6  \n745301                   5                 4  \n745302                   5                 4  \n745303                   5                 5  \n745304                   6                 3  \n\n[745305 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>MonsoonIntensity</th>\n      <th>TopographyDrainage</th>\n      <th>RiverManagement</th>\n      <th>Deforestation</th>\n      <th>Urbanization</th>\n      <th>ClimateChange</th>\n      <th>DamsQuality</th>\n      <th>Siltation</th>\n      <th>AgriculturalPractices</th>\n      <th>...</th>\n      <th>IneffectiveDisasterPreparedness</th>\n      <th>DrainageSystems</th>\n      <th>CoastalVulnerability</th>\n      <th>Landslides</th>\n      <th>Watersheds</th>\n      <th>DeterioratingInfrastructure</th>\n      <th>PopulationScore</th>\n      <th>WetlandLoss</th>\n      <th>InadequatePlanning</th>\n      <th>PoliticalFactors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1117957</td>\n      <td>4</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>8</td>\n      <td>7</td>\n      <td>8</td>\n      <td>...</td>\n      <td>8</td>\n      <td>5</td>\n      <td>7</td>\n      <td>5</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1117958</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n      <td>7</td>\n      <td>5</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>7</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1117959</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>5</td>\n      <td>7</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>9</td>\n      <td>2</td>\n      <td>5</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>6</td>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1117960</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>7</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6</td>\n      <td>7</td>\n      <td>6</td>\n      <td>4</td>\n      <td>2</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1117961</td>\n      <td>6</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>6</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6</td>\n      <td>4</td>\n      <td>6</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>745300</th>\n      <td>1863257</td>\n      <td>5</td>\n      <td>4</td>\n      <td>8</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1</td>\n      <td>3</td>\n      <td>5</td>\n      <td>6</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>745301</th>\n      <td>1863258</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>3</td>\n      <td>7</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>5</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>745302</th>\n      <td>1863259</td>\n      <td>5</td>\n      <td>7</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>6</td>\n      <td>7</td>\n      <td>5</td>\n      <td>5</td>\n      <td>...</td>\n      <td>6</td>\n      <td>11</td>\n      <td>3</td>\n      <td>11</td>\n      <td>4</td>\n      <td>5</td>\n      <td>9</td>\n      <td>5</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>745303</th>\n      <td>1863260</td>\n      <td>4</td>\n      <td>7</td>\n      <td>6</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8</td>\n      <td>6</td>\n      <td>...</td>\n      <td>6</td>\n      <td>6</td>\n      <td>8</td>\n      <td>6</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8</td>\n      <td>7</td>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>745304</th>\n      <td>1863261</td>\n      <td>4</td>\n      <td>2</td>\n      <td>5</td>\n      <td>3</td>\n      <td>8</td>\n      <td>4</td>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>6</td>\n      <td>10</td>\n      <td>4</td>\n      <td>3</td>\n      <td>9</td>\n      <td>8</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>745305 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s4e5/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-13T17:03:54.663485Z","iopub.execute_input":"2024-05-13T17:03:54.663842Z","iopub.status.idle":"2024-05-13T17:03:54.957704Z","shell.execute_reply.started":"2024-05-13T17:03:54.663814Z","shell.execute_reply":"2024-05-13T17:03:54.956704Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ntest_data_scaled = scaler.transform(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:20:46.321062Z","iopub.execute_input":"2024-05-21T19:20:46.321405Z","iopub.status.idle":"2024-05-21T19:20:49.109107Z","shell.execute_reply.started":"2024-05-21T19:20:46.321381Z","shell.execute_reply":"2024-05-21T19:20:49.108109Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.metrics import r2_score\n\n# Initialize K-Fold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nlgbm_r2_scores = []\n\nfor train_index, val_index in kf.split(X_scaled):\n    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n    y_train_fold, y_val_fold = y[train_index], y[val_index]\n    \n    # Initialize and train LightGBM model\n    lgbm = LGBMRegressor()\n    lgbm.fit(X_train_fold, y_train_fold)\n    \n    # Predict on validation set\n    lgbm_val_predictions = lgbm.predict(X_val_fold)\n    lgbm_r2 = r2_score(y_val_fold, lgbm_val_predictions)\n    lgbm_r2_scores.append(lgbm_r2)\n\nprint(f\"LightGBM Mean Validation R²: {np.mean(lgbm_r2_scores):.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:21:00.958980Z","iopub.execute_input":"2024-05-21T19:21:00.959811Z","iopub.status.idle":"2024-05-21T19:22:12.649083Z","shell.execute_reply.started":"2024-05-21T19:21:00.959780Z","shell.execute_reply":"2024-05-21T19:22:12.648050Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153427 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 623\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504511\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 623\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.153131 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504482\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.151270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504471\nLightGBM Mean Validation R²: 0.77\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import r2_score\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ndl_r2_scores = []\n\nfor train_index, val_index in kf.split(X_scaled):\n    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]\n    y_train_fold, y_val_fold = y[train_index], y[val_index]\n    \n    # Build and compile the deep learning model\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(X_train_fold.shape[1],)),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(32, activation='relu'),\n        Dropout(0.2),\n        Dense(1)\n    ])\n    model.compile(optimizer=Adam(), loss='mean_squared_error')\n\n    # Train the model\n    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=64, validation_data=(X_val_fold, y_val_fold), verbose=1)\n\n    # Predict on validation set\n    dl_val_predictions = model.predict(X_val_fold).flatten()\n    dl_r2 = r2_score(y_val_fold, dl_val_predictions)\n    dl_r2_scores.append(dl_r2)\n\nprint(f\"Deep Learning Mean Validation R²: {np.mean(dl_r2_scores):.2f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine predictions\ncombined_val_predictions = (lgbm_val_predictions + dl_val_predictions) / 2\ncombined_rmse = mean_squared_error(y_val, combined_val_predictions, squared=False)\nprint(f\"Combined Validation RMSE: {combined_rmse:.2f}\")\n\n# Train both models on the entire training data\nlgbm.fit(X, y)\nmodel.fit(scaler.transform(X), y, epochs=50, batch_size=32, verbose=1)\n\n# Make predictions on test data\nlgbm_test_predictions = lgbm.predict(test_data_scaled)\ndl_test_predictions = model.predict(test_data_scaled).flatten()\n\n# Combine test predictions\ncombined_test_predictions = (lgbm_test_predictions + dl_test_predictions) / 2\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': combined_test_predictions})\nsubmission.to_csv(\"combined_model_submission.csv\", index=False)\nprint(\"Predictions saved for the combined model\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:41:52.650095Z","iopub.execute_input":"2024-05-21T19:41:52.650469Z","iopub.status.idle":"2024-05-21T19:41:55.677642Z","shell.execute_reply.started":"2024-05-21T19:41:52.650440Z","shell.execute_reply":"2024-05-21T19:41:55.676765Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n# Initialize and train LightGBM model\nlgbm = LGBMRegressor()\nlgbm.fit(X_train_scaled, y_train)\n\n# Predict on validation set\nlgbm_val_predictions = lgbm.predict(X_val_scaled)\nlgbm_r2 = r2_score(y_val, lgbm_val_predictions)\nprint(f\"LightGBM Validation R²: {lgbm_r2:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:42:05.723284Z","iopub.execute_input":"2024-05-21T19:42:05.724169Z","iopub.status.idle":"2024-05-21T19:42:20.480810Z","shell.execute_reply.started":"2024-05-21T19:42:05.724135Z","shell.execute_reply":"2024-05-21T19:42:20.479842Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\nLightGBM Validation R²: 0.77\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Build and compile the deep learning model\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(1)\n])\nmodel.compile(optimizer=Adam(), loss='mean_squared_error')\n\n# Train the model\nmodel.fit(X_train_scaled, y_train, epochs=10, batch_size=64, validation_data=(X_val_scaled, y_val), verbose=1)\n\n# Predict on validation set\ndl_val_predictions = model.predict(X_val_scaled).flatten()\ndl_r2 = r2_score(y_val, dl_val_predictions)\nprint(f\"Deep Learning Validation R²: {dl_r2:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:42:35.390827Z","iopub.execute_input":"2024-05-21T19:42:35.391532Z","iopub.status.idle":"2024-05-21T19:46:56.686215Z","shell.execute_reply.started":"2024-05-21T19:42:35.391501Z","shell.execute_reply":"2024-05-21T19:46:56.685151Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 3.9320e-04\nEpoch 2/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.4248e-04 - val_loss: 3.8776e-04\nEpoch 3/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.2254e-04 - val_loss: 3.9074e-04\nEpoch 4/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.2037e-04 - val_loss: 4.4119e-04\nEpoch 5/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1738e-04 - val_loss: 3.8642e-04\nEpoch 6/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1644e-04 - val_loss: 4.4174e-04\nEpoch 7/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1481e-04 - val_loss: 4.1463e-04\nEpoch 8/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2ms/step - loss: 4.1463e-04 - val_loss: 3.7821e-04\nEpoch 9/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1369e-04 - val_loss: 4.0020e-04\nEpoch 10/10\n\u001b[1m13975/13975\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - loss: 4.1335e-04 - val_loss: 4.1790e-04\n\u001b[1m6988/6988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\nDeep Learning Validation R²: 0.84\n","output_type":"stream"}]},{"cell_type":"code","source":"# Combine predictions\ncombined_val_predictions = (lgbm_val_predictions + dl_val_predictions) / 2\ncombined_r2 = r2_score(y_val, combined_val_predictions)\nprint(f\"Combined Validation R²: {combined_r2:.2f}\")\n\n# Train both models on the entire training data\nlgbm.fit(X, y)\nmodel.fit(scaler.transform(X), y, epochs=10, batch_size=32, verbose=1)\n\n# Make predictions on test data\nlgbm_test_predictions = lgbm.predict(test_data_scaled)\ndl_test_predictions = model.predict(test_data_scaled).flatten()\n\n# Combine test predictions\ncombined_test_predictions = (lgbm_test_predictions + dl_test_predictions) / 2\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': combined_test_predictions})\nsubmission.to_csv(\"combined_model_submission.csv\", index=False)\nprint(\"Predictions saved for the combined model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T19:51:01.298725Z","iopub.execute_input":"2024-05-21T19:51:01.299106Z","iopub.status.idle":"2024-05-21T20:00:26.114885Z","shell.execute_reply.started":"2024-05-21T19:51:01.299075Z","shell.execute_reply":"2024-05-21T20:00:26.113848Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Combined Validation R²: 0.82\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 602\n[LightGBM] [Info] Number of data points in the train set: 1117957, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504480\nEpoch 1/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2517e-04\nEpoch 2/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2456e-04\nEpoch 3/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2446e-04\nEpoch 4/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2381e-04\nEpoch 5/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2336e-04\nEpoch 6/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1ms/step - loss: 4.2322e-04\nEpoch 7/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2248e-04\nEpoch 8/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.1930e-04\nEpoch 9/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1ms/step - loss: 4.2046e-04\nEpoch 10/10\n\u001b[1m34937/34937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1ms/step - loss: 4.2037e-04\n\u001b[1m23291/23291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step\nPredictions saved for the combined model\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom lightgbm import LGBMRegressor\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\ntest_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n\n# Separate features and target\nX = train_data.drop(columns=['FloodProbability'])\ny = train_data['FloodProbability']\n\n# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\ntest_data_scaled = scaler.transform(test_data)\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 50, 70],\n    'max_depth': [6, 8, 10],\n    'min_child_samples': [20, 30, 50],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9],\n    'reg_alpha': [0.0, 0.1, 1.0],\n    'reg_lambda': [0.0, 0.1, 1.0]\n}\n\n# Initialize the model\nlgbm = LGBMRegressor()\n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=3, scoring='r2', verbose=2, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint(\"Best parameters found: \", best_params)\n\nbest_lgbm = LGBMRegressor(**best_params)\n\n# Train the model\nbest_lgbm.fit(X_train_scaled, y_train)\n\n# Predict on validation set\nval_predictions = best_lgbm.predict(X_val_scaled)\nval_r2 = r2_score(y_val, val_predictions)\nprint(f\"Validation R²: {val_r2:.2f}\")\n\n# Make predictions on test data\ntest_predictions = best_lgbm.predict(test_data_scaled)\n\n# Save predictions to CSV\nsubmission = pd.DataFrame({'id': test_data['id'], 'FloodProbability': test_predictions})\nsubmission.to_csv(\"best_lgbm_submission.csv\", index=False)\nprint(\"Predictions saved for the best LightGBM model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T20:15:26.528735Z","iopub.execute_input":"2024-05-21T20:15:26.529111Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 19683 candidates, totalling 59049 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.336415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  33.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254496 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  39.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.315447 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  32.1s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271293 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.439177 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  30.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282345 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  32.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  37.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.322585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  31.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246913 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  32.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  43.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  35.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269029 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  39.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280345 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  31.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.312426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  31.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  34.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  35.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.340283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  34.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.346071 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  33.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.432622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  39.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306479 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  45.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263092 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  35.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  33.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.322198 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  32.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.305920 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  33.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.335912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  30.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295927 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  32.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.320644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  49.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  37.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291535 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.276069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  40.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  37.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.280974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  30.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300733 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  48.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.319126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  37.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  42.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.301361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  39.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266159 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  43.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  34.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.308964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  41.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.277582 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  32.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296551 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  36.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  30.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  31.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287108 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  36.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.287090 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  35.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299062 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  36.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.9; total time=  33.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  55.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283007 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  38.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.7; total time=  32.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  30.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256811 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time=  38.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.413854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  38.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.306849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  33.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  32.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.296807 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=0.1, subsample=0.8; total time=  35.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295199 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.7; total time=  31.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.300620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.8; total time=  43.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273799 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=0.1, reg_lambda=1.0, subsample=0.9; total time=  29.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.8; total time=  28.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.462197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.0, subsample=0.9; total time=  50.7s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.309438 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.7; total time= 1.0min\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288683 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.9; total time=  39.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  38.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  39.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314496 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  36.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.8; total time=  42.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  39.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  35.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.9; total time=  34.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.323050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  33.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.294668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260491 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  31.3s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.295903 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  35.2s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.349558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.8; total time=  35.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.279209 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.1, subsample=0.7; total time=  35.0s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=0.1, subsample=0.8; total time=  32.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.293861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.7; total time=  45.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.281302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.8; total time=  32.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.297974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=31, reg_alpha=1.0, reg_lambda=1.0, subsample=0.9; total time=  37.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.286953 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.7; total time=  35.5s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.0, subsample=0.9; total time=  51.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265132 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.7; total time=  39.9s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=0.1, subsample=0.8; total time=  34.7s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.328187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.7; total time=  39.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.8; total time=  32.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504457\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.0, reg_lambda=1.0, subsample=0.9; total time=  38.1s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.7; total time=  36.8s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 621\n[LightGBM] [Info] Number of data points in the train set: 596243, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504496\n[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=6, min_child_samples=20, n_estimators=100, num_leaves=50, reg_alpha=0.1, reg_lambda=0.0, subsample=0.9; total time=  55.4s\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 624\n[LightGBM] [Info] Number of data points in the train set: 596244, number of used features: 21\n[LightGBM] [Info] Start training from score 0.504488\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}